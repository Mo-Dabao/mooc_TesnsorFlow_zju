{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 波士顿房价预测线性回归实践\n",
    "\n",
    "按课程案例，动手完成编码实践。\n",
    "\n",
    "通过梯度下降优化器进行优化，尝试采用不同的学习率和训练轮数等超参数，记录训练后的损失值和W、b变量值。\n",
    "\n",
    "提交要求：\n",
    "\n",
    "1、至少5次不同超参数的运行结果的记录文档（word格式或者txt格式）\n",
    "\n",
    "2、你认为最优的一次带运行结果的源代码文件（.ipynb 格式）\n",
    "\n",
    "3、以上两个文件一起压缩为一个压缩文件后作为附件上传\n",
    "<br><br>\n",
    "\n",
    "评价标准：\n",
    "\n",
    "1、完成案例中的代码，有完整的代码，模型能运行优化出结果，8分；\n",
    "\n",
    "2、调整过超参数，记录文件中有至少5组数据，2分；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('boston.csv')\n",
    "y_data = df.values[:, 12]\n",
    "x_data = df.values[:, :12]\n",
    "# 对特征进行归一化处理\n",
    "x_data = (x_data - x_data.min(axis=0)) / (x_data.max(axis=0) - x_data.min(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(b):\n",
    "    tf.reset_default_graph()\n",
    "    x = tf.placeholder(tf.float32, [None, 12], name='X')  # 12个特征\n",
    "    y = tf.placeholder(tf.float32, [None, 1], name='Y')  # 标签\n",
    "    with tf.name_scope('Model'):\n",
    "        # 初始化12个特征的权重\n",
    "        w = tf.Variable(tf.random_normal([12, 1], stddev=0.01), name='W')\n",
    "        # 初始化偏置\n",
    "        b = tf.Variable(b, name='b')\n",
    "        # 多元线性模型\n",
    "        def model(x, w, b):\n",
    "            return tf.matmul(x, w) + b\n",
    "        # 向前计算节点\n",
    "        pred = model(x, w, b)\n",
    "    # 损失函数\n",
    "    with tf.name_scope('LossFunction'):\n",
    "        loss_function = tf.reduce_mean(tf.pow(y - pred, 2))  # 均方误差\n",
    "    # 梯度下降优化器\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss_function)\n",
    "    sess = tf.Session()\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    return sess, x, y, w, b, optimizer, loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索超参\n",
    "- 训练轮数train_epochs取5, 50, 100\n",
    "- 学习率learning_rates取0.1, 0.01, 0.001\n",
    "- 初始偏置b取1.0, 30.0, 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_epochs = [5, 50, 100]\n",
    "# learning_rates = [0.1, 0.01, 0.001]\n",
    "# bs = [1., 30., 100.]\n",
    "# f = open('result.txt', 'w', encoding='utf-8')\n",
    "# for learning_rate in learning_rates:\n",
    "#     for b0 in bs:\n",
    "#         sess, x, y, w, b, optimizer, loss_function = reset(b0)\n",
    "#         for epoch in range(max(train_epochs)):\n",
    "#             loss_sum = 0\n",
    "#             for xs, ys in zip(x_data, y_data):\n",
    "#                 xs = xs.reshape(1, 12)\n",
    "#                 ys = ys.reshape(1, 1)\n",
    "#                 _, loss = sess.run([optimizer, loss_function], feed_dict={x: xs, y: ys})\n",
    "#                 loss_sum += loss\n",
    "#             b_hat = b.eval(sess)\n",
    "#             w_hat = w.eval(sess)\n",
    "#             loss_average = loss_sum / len(y_data)\n",
    "#             x_data, y_data = shuffle(x_data, y_data)\n",
    "#             if epoch + 1 in train_epochs:\n",
    "#                 text = (f'超参：train_epochs = {epoch+1}, learning_rate = {learning_rate}, 初始偏置b = {b0}\\n'\n",
    "#                         + f'训练后：loss = {loss_average:.5f}, b = {b_hat:.5f},\\n'\n",
    "#                         + f'w = {list(np.around(w_hat[:, 0], decimals=2))}\\n\\n')\n",
    "#                 f.write(text.encode('utf-8').decode())\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**当训练轮数train_epochs取100，学习率learning_rates取0.01，初始偏置b取100.0时损失值最小**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参：train_epochs = 100, learning_rate = 0.01, 初始偏置b = 100.0\n",
      "训练后：loss = 23.63922, b = 31.31480,\n",
      "w = [-10.58, 4.61, 0.58, 2.82, -8.92, 19.03, 0.58, -16.5, 7.05, -6.54, -8.7, -20.03]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "train_epochs = 100\n",
    "b0 = 100.0\n",
    "sess, x, y, w, b, optimizer, loss_function = reset(b0)\n",
    "for epoch in range(train_epochs):\n",
    "    loss_sum = 0\n",
    "    for xs, ys in zip(x_data, y_data):\n",
    "        xs = xs.reshape(1, 12)\n",
    "        ys = ys.reshape(1, 1)\n",
    "        _, loss = sess.run([optimizer, loss_function], feed_dict={x: xs, y: ys})\n",
    "        loss_sum += loss\n",
    "    b_hat = b.eval(sess)\n",
    "    w_hat = w.eval(sess)\n",
    "    loss_average = loss_sum / len(y_data)\n",
    "    x_data, y_data = shuffle(x_data, y_data)\n",
    "print(f'超参：train_epochs = {epoch+1}, learning_rate = {learning_rate}, 初始偏置b = {b0}\\n'\n",
    "      + f'训练后：loss = {loss_average:.5f}, b = {b_hat:.5f},\\n'\n",
    "      + f'w = {list(np.around(w_hat[:, 0], decimals=2))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2]",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
